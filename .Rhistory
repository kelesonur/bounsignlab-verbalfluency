}
pdftotext <- "C:/Users/keles/Desktop/pdftotext.exe"
for(i in 1:length(dir(folder)))
{
pdf <- file.path("C:/Users/keles/Desktop/2020-2021 EE Academy/OSYM Corpus", dirpdf[1])
system(paste("\"", pdftotext, "\" \"", sep = ""),wait = F )
}
folder <- file.path("C:/Users/keles/Desktop/2020-2021 EE Academy/OSYM Corpus")
folder
length <- length(dir(folder))
length
dirpdf <- dir(folder)
dirpdf[1]
pdftotext <- "C:/Users/keles/Desktop/pdftotext.exe"
for(i in 1:length(dir(folder)))
{
pdf <- file.path("C:/Users/keles/Desktop/2020-2021 EE Academy/OSYM Corpus", dirpdf[1])
system(paste("\"", pdftotext, "\" \"", sep = ""),wait = F )
}
folder <- file.path("C:/Users/keles/Desktop/2020-2021 EE Academy/OSYM Corpus")
folder
length <- length(dir(folder))
length
dirpdf <- dir(folder)
dirpdf[1]
pdftotext <- "C:/Users/keles/Desktop/pdftotext.exe"
for(i in 1:length(dir(folder)))
{
pdf <- file.path("C:/Users/keles/Desktop/pdf", dirpdf[1])
system(paste("\"", pdftotext, "\" \"", sep = ""),wait = F )
}
folder <- file.path("C:/Users/keles/Desktop/2020-2021 EE Academy/OSYM Corpus")
folder
length <- length(dir(folder))
length
dirpdf <- dir(folder)
dirpdf[1]
pdftotext <- "C:/Users/keles/Desktop/pdftotext.exe"
for(i in 1:length(dir(folder)))
{
pdf <- file.path("C:/Users/keles/Desktop/pdf", dirpdf[1])
system(paste("\"", pdftotext, "\" \"", sep = ""),wait = F )
}
folder <- file.path("C:/Users/keles/Desktop/2020-2021 EE Academy/OSYM Corpus")
folder
length <- length(dir(folder))
length
dirpdf <- dir(folder)
dirpdf[1]
pdftotext <- "C:/Users/keles/Desktop/pdftotext.exe"
for(i in 1:length(dir(folder)))
{
pdf <- file.path("C:/Users/keles/Desktop/2020-2021 EE Academy/OSYM Corpus", dirpdf[1])
system(paste("\"", pdftotext, "\" \"", sep = ""),wait = F )
}
pdf2html
# folder with 1000s of PDFs
dest <- "C:/Users/keles/Desktop/2020-2021 EE Academy/OSYM Corpus"
# make a vector of PDF file names
myfiles <- list.files(path = dest, pattern = "pdf",  full.names = TRUE)
# convert each PDF file that is named in the vector into a text file
# text file is created in the same directory as the PDFs
# note that my pdftotext.exe is in a different location to yours
lapply(myfiles, function(i) system(paste('"C:/Users/keles/Desktop/pdftotext.exe"',
paste0('"', i, '"')), wait = FALSE) )
# folder with 1000s of PDFs
dest <- "C:/Users/keles/Desktop/2020-2021 EE Academy/OSYM Corpus"
# make a vector of PDF file names
myfiles <- list.files(path = dest, pattern = "pdf",  full.names = TRUE)
# convert each PDF file that is named in the vector into a text file
# text file is created in the same directory as the PDFs
# note that my pdftotext.exe is in a different location to yours
lapply(myfiles, function(i) system(paste('"C:/Users/keles/Desktop/pdftotext.exe"',
paste0('"', i, '"')), wait = FALSE) )
# folder with 1000s of PDFs
dest <- "C:/Users/keles/Desktop/2020-2021 EE Academy/OSYM Corpus"
# make a vector of PDF file names
myfiles <- list.files(path = dest, pattern = "pdf",  full.names = TRUE)
# convert each PDF file that is named in the vector into a text file
# text file is created in the same directory as the PDFs
# note that my pdftotext.exe is in a different location to yours
lapply(myfiles, function(i) system(paste('"C:/Users/keles/Desktop/pdftotext.exe"',
paste0('"', i, '"')), wait = FALSE) )
dest <- "C:/Users/keles/Desktop/2020-2021 EE Academy/OSYM Corpus"
myfiles <- list.files(path = dest, pattern = "pdf",  full.names = TRUE)
myfiles
dest
myfiles <- list.files(path = dest, pattern = ".pdf",  full.names = TRUE)
myfiles
myfiles <- list.files(path = "C:/Users/keles/Desktop/2020-2021 EE Academy/OSYM Corpus", pattern = "pdf",  full.names = TRUE)
myfiles
setwd("C:/Users/keles/Desktop/2020-2021 EE Academy/OSYM Corpus")
getwd(
)
setwd("C:/Users/keles/Desktop/2020-2021 EE Academy/OSYM Corpus")
# folder with 1000s of PDFs
dest <- "C:/Users/keles/Desktop/2020-2021 EE Academy/Corpus"
dest
# make a vector of PDF file names
myfiles <- list.files(path = dest, pattern = "pdf",  full.names = TRUE)
myfiles
# convert each PDF file that is named in the vector into a text file
# text file is created in the same directory as the PDFs
# note that my pdftotext.exe is in a different location to yours
lapply(myfiles, function(i) system(paste('"C:/Users/keles/Desktop/pdftotext.exe"',
paste0('"', i, '"')), wait = FALSE) )
# folder with 1000s of PDFs
dest <- "C:/Users/keles/Desktop/2020-2021 EE Academy/Corpus"
dest
# make a vector of PDF file names
myfiles <- list.files(path = dest, pattern = "pdf",  full.names = TRUE)
myfiles
# convert each PDF file that is named in the vector into a text file
# text file is created in the same directory as the PDFs
# note that my pdftotext.exe is in a different location to yours
lapply(myfiles, function(i) system(paste('"C:/Users/keles/Desktop/pdftotext.exe"',
paste0('"', i, '"')), wait = FALSE) )
library(magrittr)
# Load the package 'languageR' to get access to the dataset 'dative' (not 'dativeSimplified')
install.packages("languageR")
library("languageR")
# Take a look at the first few rows of 'dative' to make sure it's actually loaded
head(dative)
# Compute the median of the column 'LengthOfTheme'.
# (A median is the central-most value in a vector. The function to compute it is called median()).
median(dative$LengthOfTheme)
df <- dative %>%
mutate(long_theme = ifelse(LengthOfTheme > median(LengthOfRecipient), "large","short"))
df <- dative %>%
dplyr::mutate(long_theme = ifelse(LengthOfTheme > median(LengthOfRecipient), "large","short"))
df
head(df)
df <- dative %>%
dplyr::mutate(long_theme = ifelse(LengthOfTheme > median(LengthOfRecipient), "large","short"))
group_by(long_theme) %>%
dplyr::summarize(percNC = mean(RealizationOfRecipient == "NP"))
df <- dative %>%
dplyr::mutate(long_theme = ifelse(LengthOfTheme > median(LengthOfRecipient), "large","short"))
dplyr::group_by(long_theme) %>%
dplyr::summarize(percNC = mean(RealizationOfRecipient == "NP"))
df <- dative %>%
dplyr::mutate(long_theme = ifelse(LengthOfTheme > median(LengthOfRecipient), "large","short")) %>%
dplyr::group_by(long_theme) %>%
dplyr::summarize(percNC = mean(RealizationOfRecipient == "NP"))
# Use group_by() + summarize() to compute the proportion of NP realizations of the recipient as a function of 'long_theme'.
View(df)
df$long_theme
df$long_theme -> ifelse(LengthOfTheme > median(LengthOfRecipient), "large","short"))
df$long_theme -> ifelse(LengthOfTheme > median(LengthOfRecipient), "large","short")
df$long_theme -> ifelse(LengthOfTheme > median(LengthOfRecipient), "large","short")
df$long_theme -> ifelse(dative$LengthOfTheme > median(LengthOfRecipient), "large","short")
df$long_theme -> ifelse(dative$LengthOfTheme > median(dative$LengthOfRecipient), "large","short")
median(dative$LengthOfRecipient)
df <- dative %>%
dplyr::mutate(long_theme = ifelse(LengthOfTheme > median(LengthOfTheme), "large","short")) %>%
dplyr::group_by(long_theme) %>%
dplyr::summarize(percNC = mean(RealizationOfRecipient == "NP"))
View(df)
df$long_theme <- ifelse(dative$LengthOfTheme > median(dative$LengthOfRecipient), "large","short")
df$long_theme <- ifelse(dative$LengthOfTheme > median(dative$LengthOfTheme), "large","short")
ifelse[s]
df %>% dplyr::group_by(long_theme) %>%
dplyr::summarize(percNC = mean(RealizationOfRecipient == "NP"))
df <- dative %>%
dplyr::mutate(long_theme = ifelse(LengthOfTheme > median(LengthOfTheme), "large","short"))
Use group_by() + summarize() to compute the proportion of NP realizations of the recipient as a function of 'long_theme'.
df %>% dplyr::group_by(long_theme) %>%
dplyr::summarize(percNC = mean(RealizationOfRecipient == "NP"))
library(ggplot2)
df <- data.frame(a=1:10, a_sq=(1:10)^2, group = c("a","b"))
df
ggplot(df, aes(a,a_sq)) +
geom_point()
ggplot(df, aes(a,a_sq)) +
geom_bar(stat = "identity")
ggplot(df, aes(a,a_sq, color=group)) +
geom_point()
ggplot(df, aes(a,a_sq, color=group)) +
geom_line() +
facet_wrap(~group)
ggplot(df, aes(a,a_sq)) +
geom_line() +
facet_wrap(group)
# Take a look at the documentation of the data frame 'population'
help(population)
# Take a look at the documentation of the data frame 'population'
?help(population)
# Using unique(), find out which countries we have data for
unique(population)
# Take a look at the documentation of the data frame 'population'
help(population)
# Take a look at the documentation of the data frame 'population'
??population
# Take a look at the documentation of the data frame 'population'
help(population)
population
library(tidyverse)
install.packages("tidyverse")
library(tidyverse)
# Take a look at the documentation of the data frame 'population'
help(population)
# Using unique(), find out which countries we have data for
unique(country)
# Using unique(), find out which countries we have data for
unique(population)
# Using subset(), extract the data for Turkey and Ukraine and save it in data frames called df_turkey and df_ukraine, respectively
?subset()
subset(population, country == Turkey)
df_Turkey <- subset(population, country == Turkey)
df_Turkey <- subset(population, country == "Turkey")
df_Ukraine <- subset(population, country == "Ukraine")
# Use View() to inspect them
View(df_Turk)
# Use View() to inspect them
View(df_Turkey)
# Let R print the year column in df_turkey to see for which years data is available
df_Turkey$year
# load ggplot2
library(ggplot2)
ggplot(df_Turkey, aes(country, year)+
geom_point)
# Use View() to inspect them
View(df_Turkey)
ggplot(df_Turkey, aes(year, population)+
geom_point)
ggplot(df_Turkey, aes(year, population)+
geom_point()
ggplot(df_Turkey, aes(year, population) +
geom_point()
# Use View() to inspect them
View(df_Turkey)
ggplot(df_Turkey, aes(x= year, y= population) +
geom_point()
library(ggplot2)
ggplot(df_Turkey, aes(x= year, y= population) +
geom_point()
ggplot(df_Turkey, aes(x= year, y= population) +
geom_point())
ggplot(df_Turkey, aes(x= year, y= population) +
geom_point()
)
ggplot(df_Turkey, aes(x= year, y= population)) +
geom_point()
ggplot(df_Ukraine, aes(x= year, y= population)) +
geom_point()
# Using rbind(), create a single data frame called df_trukr containing both datasets
rbind(df_Turkey, df_Ukraine)
# Using rbind(), create a single data frame called df_trukr containing both datasets
df_trukr <- rbind(df_Turkey, df_Ukraine)
ggplot(df_trukr, aes(x= year, y= population, color = country)) +
geom_bar(stat = "identity")
ggplot(df_trukr, aes(x= year, y= population, fill = country)) +
geom_bar(stat = "identity")
ggplot(df_trukr, aes(x= year, y= population, color = country)) +
geom_point()
ggplot(df_trukr, aes(x= year, y= population)) +
geom_point() +
facet_wrap(~country)
df_trukr
change <-df_trukr %>%
group_by(country) %>%
summarize(population[38] - population[1])
change
population[38]
df_trukr$population[38]
change <-df_trukr %>%
group_by(country) %>%
summarize(df_trukr$population[38] - df_trukr$population[1])
change
(df_trukr$population[38]
df_trukr$population[38]
df_trukr$population
change <-df_trukr %>%
group_by(country) %>%
summarize(df_trukr$population[1] - df_trukr$population[38])
change
# plot both in one plot, in two facets
ggplot(df_trukr, aes(x= year, y= population, color = country)) +
geom_point() + geom_line()
facet_wrap(~country)
# plot both in one plot, in two facets
ggplot(df_trukr, aes(x= year, y= population, fill = country)) +
geom_point() +
geom_line() +
facet_wrap(~country)
# plot both in one plot, in two facets
ggplot(df_trukr, aes(x= year, y= population, color = country)) +
geom_point() +
geom_line() +
facet_wrap(~country)
danish_summary
###
### Please consider this assignment a type of translation task. Translate each of the
### statements below from English to R. Each instruction should correspond to 1-3 lines
### of code (usually one line).
### Please note that there may be multiple, equally valid solutions to each instruction.
###
### After you finish the assignment, please click on "Session"->"Restart R" in your R Studio
### and run the code again, to make sure it executes properly in *exactly* the order in which
### you have written it. (The most common error is that people don't load packages later than
### they should be loaded, or leave install.packages() calls in this R code. )
###
### Each statement is worth 1 point.
###
# subtract 15 from 1
1 - 15
# create a vector called my_vector, and fill it with all integers from 1 to 10
my_vector <- 1L:10L
# add 5 to my_vector
my_vector + 5
# determine which elements of the expression (my_vector^10) are smaller than 50
my_vector^10 < 50
my_vector[my_vector^10 < 50]
# create a vector called z containing the values 8, 18, and 38
z <- c(8,18,38)
# using the vector called letters which is already available in R, extract the 6-th,7-th, and 9th letter
letters[c(6,7,9)]
# Create a data frame called df with the columns A and B. Let A contain the numbers from 1 to 5, and B the letters from 'a' to 'e'.
# (Use the function data.frame().)
df <- data.frame(A = 1:5, B = letters[1:5])
# load the package called 'languageR' and take a look at the first few columns of the data set called danish
library(languageR)
head(danish)
# use the function help() to look up the documentation
help(danish)
# load the package dplyr, and compute the average value of the column Log-RT by sex (look up the actual name of the columns in the dataset documentation)
library(dplyr)
danish_summary <- danish %>%
group_by(Sex) %>%
summarise(mean_LogRT = mean(LogRT)
)
danish_summary
setwd("C:\Users\keles\OneDrive\Belgeler\GitHub\bounsignlab-verbalfluency")
setwd("C:/Users/keles/OneDrive/Belgeler/GitHub/bounsignlab-verbalfluency")
######################################### CORRECT RESPONSES ANALYSIS #########################################
df_ncr <- readRDS("df_ncr.rds")
# read the data frame, drop NA values
df <- read_csv("vf_data.csv") %>% drop_na()
setwd("C:/Users/keles/OneDrive/Belgeler/GitHub/bounsignlab-verbalfluency")
library(dplyr)
library(magrittr)
library(tidyr)
library(readxl)
library(gdata)
library(stringr)
setwd("C:/Users/keles/OneDrive/Belgeler/GitHub/bounsignlab-verbalfluency")
# read the data frame, drop NA values
df <- read_csv("vf_data.csv") %>% drop_na()
# encode vector types
df$subject %<>% as.integer()
df$group %<>% dplyr::recode(`0` = "Late", `1` = "Native") %>% as.factor() %>% reorder.factor(new.order = c("Native","Late"))
df$item %<>% as.integer()
df$difficulty %<>% as.factor()
df$word_nr %<>% as.integer()
df$onset %<>% as.integer()
df$offset %<>% as.integer()
df$category %<>% as.factor()
df$type %<>% as.factor()
df$cat2 <- with(df, case_when(item %in% c(1:6) ~ "HS",
item %in% c(7:12) ~ "LOC",
item %in% c(13:18) ~ "SEM")) %>% as.factor()
saveRDS(df,"df.rds")
### DATA AGGREGATION FOR ANALYSIS ###
# correct responses for analysis, order by subject and onset
df_correct <- df %>% subset(type =="Correct") %>% dplyr::select(-type, -offset, -word_nr)
# read the data frame, drop NA values
df <- read_csv("vf_data.csv") %>% drop_na()
# read the data frame, drop NA values
df <- read_csv("vf_data.csv") %>% drop_na()
library(dplyr)
library(magrittr)
library(tidyr)
library(readxl)
library(gdata)
library(stringr)
# read the data frame, drop NA values
df <- read_csv("vf_data.csv") %>% drop_na()
library(readr)
setwd("C:/Users/keles/OneDrive/Belgeler/GitHub/bounsignlab-verbalfluency")
# read the data frame, drop NA values
df <- read_csv("vf_data.csv") %>% drop_na()
# encode vector types
df$subject %<>% as.integer()
df$group %<>% dplyr::recode(`0` = "Late", `1` = "Native") %>% as.factor() %>% reorder.factor(new.order = c("Native","Late"))
df$item %<>% as.integer()
df$difficulty %<>% as.factor()
df$word_nr %<>% as.integer()
df$onset %<>% as.integer()
df$offset %<>% as.integer()
df$category %<>% as.factor()
df$type %<>% as.factor()
df$cat2 <- with(df, case_when(item %in% c(1:6) ~ "HS",
item %in% c(7:12) ~ "LOC",
item %in% c(13:18) ~ "SEM")) %>% as.factor()
### DATA AGGREGATION FOR ANALYSIS ###
# correct responses for analysis, order by subject and onset
df_correct <- df %>% subset(type =="Correct") %>% dplyr::select(-type, -offset, -word_nr)
df_correct %<>% dplyr::arrange(subject,onset)
# calculate ms difference between responses
df %<>% group_by(subject,item) %>% mutate(difference = onset - lag(onset, default = onset[1]))
df %<>% group_by(subject,item) %>% mutate(time_interval = cumsum(difference))
df_correct %<>% group_by(subject,item) %>% mutate(difference = onset - lag(onset, default = onset[1]))
df_correct %<>% group_by(subject,item) %>% mutate(time_interval = cumsum(difference))
df_correct %<>% dplyr::select(-onset)
# calculate how many correct responses there are
df_correct %<>% group_by(subject,item) %>% mutate(ncr = n())
df_correct %<>% group_by(subject,item) %>% mutate(n_correct = 1:n())
# calculate time seconds and slots
# df_correct$time_sec <- round((df_correct$time_interval/1000), digits = 0) %>% as.integer()
df_correct$time <- with(df_correct,
case_when(
time_interval >= 0  & time_interval < 10000 ~ "10",
time_interval >= 10000 & time_interval < 20000 ~ "20",
time_interval >= 20000 & time_interval < 30000 ~ "30",
time_interval >= 30000 & time_interval < 40000 ~ "40",
time_interval >= 40000 & time_interval < 50000 ~ "50",
T ~ "60")) %>% as.integer()
# total number of responses for each time period
df_correct %<>% group_by(subject, item, time) %>% mutate(time_total = n()) %>% ungroup()
df_correct %<>% mutate(latency_ms = time_interval - (time*1000) + 10000)
saveRDS(df_correct, "df_correct.rds")
### MODEL DATA FRAMES ###
# data frame for number of responses model
df_ncr <- df_correct %>%
dplyr::distinct(subject,item, .keep_all = T) %>%
dplyr::arrange(subject,item) %>% dplyr::select(subject, group, item, category, difficulty, ncr, cat2) %>% ungroup()
saveRDS(df_ncr, "df_ncr.rds")
# data frame for time course analysis
df_time <- df_correct %>% dplyr::distinct(subject,item, time, .keep_all = T) %>%
dplyr::arrange(subject,item,time) %>% dplyr::select(-difference, -time_interval, -ncr, -n_correct, -latency_ms) %>% ungroup()
df_time %<>% group_by(subject,item) %>% mutate(time_cum = cumsum(time_total))
saveRDS(df_time,"df_time.rds")
# fill empty intervals for cumulative reading
df_cum_time <- df_time %>% arrange(subject, item, time)
for (each in unique(df_cum_time$subject)){
for(every in unique(df_cum_time$item)){
x <- df_cum_time %>% subset(subject == each & item == every)
for(all in c(1:6)){
x %<>% arrange(subject,item, time)
time = all*10
if (time != x$time[all] || is.na(x$time[all])){
y <- tibble(subject = x$subject[1], group = x$group[1], item = x$item[1], category = x$category[1],
difficulty = x$difficulty[1], cat2 = x$cat2[1], time = time, time_total = 0, time_cum = x$time_cum[all-1])
df_cum_time %<>% rbind(y)
x %<>% rbind(y)
}
next
}
}
}
df_cum_time %<>% arrange(subject,item,time) %>% drop_na()
saveRDS(df_cum_time, "df_cum_time.rds")
# mean latency ms
df_latency <- df_correct %>% group_by(subject, item) %>% slice(2) %>%
dplyr::select(subject,item, first_ms = latency_ms)
df_latency <- left_join(df_correct, df_latency) %>% mutate(srt = (time_interval - first_ms)/1000) %>% subset(srt > 0)
saveRDS(df_latency, "df_latency.rds")
library(dplyr)
library(magrittr)
library(tidyr)
library(gdata)
library(stringr)
library(brms)
library(MASS)
library(bayesplot)
library(tidybayes)
setwd("C:/Users/keles/OneDrive/Belgeler/GitHub/bounsignlab-verbalfluency")
######################################### CORRECT RESPONSES ANALYSIS #########################################
df_ncr <- readRDS("df_ncr.rds")
### contrast coding for the predictors ###
contrasts(df_ncr$category)
contrasts(df_ncr$category) <- contr.sdif(2)
contrasts(df_ncr$category)
contrasts(df_ncr$cat2)
contrasts(df_ncr$cat2) <- contr.sum(3)/2
contrasts(df_ncr$cat2)
contrasts(df_ncr$difficulty)
contrasts(df_ncr$difficulty) <- contr.sdif(3)
contrasts(df_ncr$difficulty)
# reorder factor for contrast setting
df_ncr$group %<>% reorder.factor(new.order = c("Late","Native"))
contrasts(df_ncr$group)
contrasts(df_ncr$group) <- contr.sdif(2)
contrasts(df_ncr$group)
# model responses
ncr_model <- brm(ncr ~ cat2*difficulty*group,
family = poisson(link="log"), data = df_ncr,
chains = 4, cores = 4, iter= 3000, warmup = 2000, file = "ncr_model")
summary(ncr_model)
exp(2.15)
exp(0.25)
exp(-0.31)
exp(0.3)
exp(coef(ncr_model))
exp(fixef(ncr_model))
# model responses
ncr_model2 <- brm(ncr ~ group
family = poisson(link="log"), data = df_ncr,
chains = 4, cores = 4, iter= 3000, warmup = 2000, file = "ncr_model2")
# model responses
ncr_model <- brm(ncr ~ group
family = poisson(link="log"), data = df_ncr,
chains = 4, cores = 4, iter= 3000, warmup = 2000, file = "ncr_model")
# model responses
ncr_model <- brm(ncr ~ cat2*difficulty*group,
family = poisson(link="log"), data = df_ncr,
chains = 4, cores = 4, iter= 3000, warmup = 2000, file = "ncr_model")
# model responses
ncr_model2 <- brm(ncr ~ group,
family = poisson(link="log"), data = df_ncr,
chains = 4, cores = 4, iter= 3000, warmup = 2000, file = "ncr_model2")
exp(fixef(ncr_model2))
